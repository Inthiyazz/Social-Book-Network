{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inthiyazz/Social-Book-Network/blob/main/oral_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd5bHF9Cw9m_"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Module 1: Install Dependencies\n",
        "# ==============================\n",
        "!pip install tensorflow --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS2neXOlxMHu"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Module 2: Import Libraries\n",
        "# ==============================\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from IPython.display import display\n",
        "from ipywidgets import FileUpload\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN-jxkzQxOqr",
        "outputId": "ee10c48c-92c1-43dc-db23-13ef706e09d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading dataset...\n",
            "âœ… Download complete.\n",
            "ğŸ“¦ Extracting dataset...\n",
            "âœ… Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Module 3: Download & Extract Dataset\n",
        "# ==============================\n",
        "DATASET_URL = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/mhjyrn35p4-2.zip\"\n",
        "ZIP_FILE = \"oral_images.zip\"\n",
        "DATASET_DIR = \"dataset\"\n",
        "\n",
        "def download_dataset():\n",
        "    if not os.path.exists(ZIP_FILE):\n",
        "        print(\"ğŸ“¥ Downloading dataset...\")\n",
        "        r = requests.get(DATASET_URL, stream=True)\n",
        "        with open(ZIP_FILE, \"wb\") as f:\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"ZIP file already exists, skipping download.\")\n",
        "\n",
        "def extract_dataset():\n",
        "    if not os.path.exists(\"oral_images\"):\n",
        "        print(\"ğŸ“¦ Extracting dataset...\")\n",
        "        with zipfile.ZipFile(ZIP_FILE, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"oral_images\")\n",
        "        print(\"Extraction complete.\")\n",
        "    else:\n",
        "        print(\" Dataset already extracted.\")\n",
        "\n",
        "download_dataset()\n",
        "extract_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeUSapNYxRD_",
        "outputId": "e91b888d-6e5b-4be4-b94f-f8d6a8f3af30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Organizing dataset into train/test...\n",
            "âœ… Dataset organized successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Module 4: Organize Dataset\n",
        "# ==============================\n",
        "def create_folders():\n",
        "    for split in [\"train\", \"test\"]:\n",
        "        for cls in [\"Cancer\", \"Non-Cancer\"]:\n",
        "            os.makedirs(os.path.join(DATASET_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "def move_images(src_dir, train_dir, test_dir, split_ratio=0.8):\n",
        "    images = [f for f in os.listdir(src_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "    random.shuffle(images)\n",
        "    split_idx = int(len(images) * split_ratio)\n",
        "    for img in images[:split_idx]:\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(train_dir, img))\n",
        "    for img in images[split_idx:]:\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(test_dir, img))\n",
        "\n",
        "def split_dataset():\n",
        "    print(\" Organizing dataset into train/test...\")\n",
        "    benign_path, malignant_path = None, None\n",
        "    for root, dirs, files in os.walk(\"oral_images\"):\n",
        "        for d in dirs:\n",
        "            if \"Benign\" in d or \"benign\" in d:\n",
        "                benign_path = os.path.join(root, d)\n",
        "            elif \"Malignant\" in d or \"malignant\" in d:\n",
        "                malignant_path = os.path.join(root, d)\n",
        "    if not benign_path or not malignant_path:\n",
        "        raise Exception(\"Could not find Benign/Malignant folders in dataset.\")\n",
        "\n",
        "    move_images(benign_path, os.path.join(DATASET_DIR, \"train\", \"Non-Cancer\"),\n",
        "                os.path.join(DATASET_DIR, \"test\", \"Non-Cancer\"))\n",
        "    move_images(malignant_path, os.path.join(DATASET_DIR, \"train\", \"Cancer\"),\n",
        "                os.path.join(DATASET_DIR, \"test\", \"Cancer\"))\n",
        "    print(\" Dataset organized successfully.\")\n",
        "\n",
        "create_folders()\n",
        "split_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml7suMl4xTKg",
        "outputId": "c1d2b8e0-6da5-4dd9-b5b2-8e0163064bad"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1816 images belonging to 2 classes.\n",
            "Found 454 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m51877672/51877672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6323 - loss: 3.0217"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - accuracy: 0.6335 - loss: 2.9938 - val_accuracy: 0.8370 - val_loss: 0.3928\n",
            "Epoch 2/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 3s/step - accuracy: 0.7800 - loss: 0.4759 - val_accuracy: 0.8370 - val_loss: 0.3519\n",
            "Epoch 3/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8115 - loss: 0.4073"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.8114 - loss: 0.4072 - val_accuracy: 0.8678 - val_loss: 0.2995\n",
            "Epoch 4/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8117 - loss: 0.4074"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8119 - loss: 0.4072 - val_accuracy: 0.8811 - val_loss: 0.2758\n",
            "Epoch 5/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8304 - loss: 0.3620"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8305 - loss: 0.3616 - val_accuracy: 0.8855 - val_loss: 0.2749\n",
            "Epoch 6/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 3s/step - accuracy: 0.8419 - loss: 0.3247 - val_accuracy: 0.8833 - val_loss: 0.2609\n",
            "Epoch 7/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8471 - loss: 0.3386"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8468 - loss: 0.3387 - val_accuracy: 0.9097 - val_loss: 0.2171\n",
            "Epoch 8/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 3s/step - accuracy: 0.8623 - loss: 0.3259 - val_accuracy: 0.8260 - val_loss: 0.3682\n",
            "Epoch 9/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8072 - loss: 0.3700 - val_accuracy: 0.8965 - val_loss: 0.2543\n",
            "Epoch 10/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8462 - loss: 0.3212"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8462 - loss: 0.3213 - val_accuracy: 0.9119 - val_loss: 0.2198\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# Module 5: Train DenseNet169 Model\n",
        "# ==============================\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, \"train\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, \"test\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Base model\n",
        "base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom layers\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "checkpoint = ModelCheckpoint(\"model/densenet169_binary_classifier.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sodgz-wLxYUj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "9ef7b64f-59a8-4a36-e111-0a5cabef9bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved as final_densenet169_model.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79fb976e-81b0-4a39-8e72-99d95e68cb8e\", \"final_densenet169_model.h5\", 93316016)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ==============================\n",
        "# Module 6: Save & Download Model\n",
        "# ==============================\n",
        "model.save(\"final_densenet169_model.h5\")\n",
        "print(\"âœ… Model saved as final_densenet169_model.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"final_densenet169_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DhPSjhftrNs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "684b9c00-6573-4b0f-afbb-05396c167649"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4227991252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_densenet169_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get expected input shape (e.g., 128x128x3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (height, width)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load model once\n",
        "model = load_model(\"final_densenet169_model.h5\")\n",
        "\n",
        "# Get expected input shape (e.g., 128x128x3)\n",
        "input_shape = model.input_shape[1:3]  # (height, width)\n",
        "\n",
        "# Upload widget\n",
        "upload = FileUpload(accept='image/*', multiple=False)\n",
        "display(upload)\n",
        "\n",
        "def handle_upload(change):\n",
        "    for name, file_info in upload.value.items():\n",
        "        # Resize to match model input\n",
        "        img = Image.open(io.BytesIO(file_info['content'])).resize(input_shape)\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array /= 255.0\n",
        "\n",
        "        # Predict\n",
        "        prediction = model.predict(img_array)[0][0]  # Extract scalar value\n",
        "\n",
        "        # Decode label\n",
        "        label = \"ğŸ©ºNonCancerous\" if prediction >= 0.5 else \"Cancerous\"\n",
        "        confidence = round(prediction * 100, 2) if prediction >= 0.5 else round((1 - prediction) * 100, 2)\n",
        "\n",
        "        # Output\n",
        "        print(f\"Prediction: {label}\")\n",
        "        print(f\"Confidence: {confidence}%\")\n",
        "\n",
        "# Trigger prediction when file is uploaded\n",
        "upload.observe(handle_upload, names='value')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2dIub6gSxJog5A+36bhzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}